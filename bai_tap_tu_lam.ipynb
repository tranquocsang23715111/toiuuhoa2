{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# B√ÄI T·∫¨P T·ª∞ L√ÄM: GRADIENT DESCENT\n",
    "\n",
    "## üéØ M·ª•c ti√™u\n",
    "- T·ª± tri·ªÉn khai thu·∫≠t to√°n Gradient Descent t·ª´ ƒë·∫ßu\n",
    "- Th·ª≠ nghi·ªám v·ªõi c√°c h√†m s·ªë kh√°c nhau\n",
    "- Ph√¢n t√≠ch v√† so s√°nh k·∫øt qu·∫£\n",
    "- R√∫t ra k·∫øt lu·∫≠n v·ªÅ hi·ªáu qu·∫£ c·ªßa thu·∫≠t to√°n\n",
    "\n",
    "## ‚è∞ Th·ªùi gian: 60 ph√∫t\n",
    "## üìä ƒêi·ªÉm: 10 ƒëi·ªÉm\n",
    "\n",
    "---\n",
    "\n",
    "## üìã H∆∞·ªõng d·∫´n n·ªôp b√†i\n",
    "1. Ho√†n th√†nh t·∫•t c·∫£ c√°c TODO trong notebook n√†y\n",
    "2. Ch·∫°y t·∫•t c·∫£ c√°c cell v√† ƒë·∫£m b·∫£o kh√¥ng c√≥ l·ªói\n",
    "3. Tr·∫£ l·ªùi c√°c c√¢u h·ªèi ·ªü cu·ªëi b√†i\n",
    "4. L∆∞u file v·ªõi t√™n: `HoTen_MSSV_GradientDescent.ipynb`\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ph·∫ßn 1: C√†i ƒë·∫∑t v√† Import th∆∞ vi·ªán (1 ƒëi·ªÉm)\n",
    "\n",
    "**TODO 1.1:** C√†i ƒë·∫∑t v√† import c√°c th∆∞ vi·ªán c·∫ßn thi·∫øt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO 1.1: Import c√°c th∆∞ vi·ªán c·∫ßn thi·∫øt\n",
    "# G·ª£i √Ω: numpy, matplotlib.pyplot, seaborn (t√πy ch·ªçn)\n",
    "\n",
    "# Vi·∫øt code c·ªßa b·∫°n ·ªü ƒë√¢y:\n",
    "import numpy as no\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "\n",
    "\n",
    "# C·∫•u h√¨nh matplotlib ƒë·ªÉ v·∫Ω ƒë·ªì th·ªã ƒë·∫πp h∆°n\n",
    "# TODO: Thi·∫øt l·∫≠p figure size v√† font size ph√π h·ª£p\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ph·∫ßn 2: Tri·ªÉn khai Gradient Descent c∆° b·∫£n (3 ƒëi·ªÉm)\n",
    "\n",
    "**TODO 2.1:** Tri·ªÉn khai h√†m s·ªë v√† ƒë·∫°o h√†m\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test h√†m s·ªë:\n",
      "f(0) = 7 (k·∫øt qu·∫£ ƒë√∫ng: 7)\n",
      "f(2) = 3 (k·∫øt qu·∫£ ƒë√∫ng: 3)\n",
      "df_dx(0) = -4 (k·∫øt qu·∫£ ƒë√∫ng: -4)\n",
      "df_dx(2) = 0 (k·∫øt qu·∫£ ƒë√∫ng: 0)\n"
     ]
    }
   ],
   "source": [
    "# TODO 2.1: ƒê·ªãnh nghƒ©a h√†m s·ªë f(x) = x^2 - 4x + 7\n",
    "def f(x):\n",
    "    \"\"\"\n",
    "    H√†m s·ªë f(x) = x^2 - 4x + 7\n",
    "    \n",
    "    Args:\n",
    "        x: Gi√° tr·ªã ƒë·∫ßu v√†o\n",
    "    \n",
    "    Returns:\n",
    "        Gi√° tr·ªã c·ªßa h√†m s·ªë t·∫°i x\n",
    "    \"\"\"\n",
    "    \n",
    "    return x**2 -4*x +7\n",
    "\n",
    "# TODO 2.2: T√≠nh ƒë·∫°o h√†m f'(x) = 2x - 4\n",
    "def df_dx(x):\n",
    "    \"\"\"\n",
    "    ƒê·∫°o h√†m c·ªßa f(x)\n",
    "    \n",
    "    Args:\n",
    "        x: Gi√° tr·ªã ƒë·∫ßu v√†o\n",
    "    \n",
    "    Returns:\n",
    "        Gi√° tr·ªã ƒë·∫°o h√†m t·∫°i x\n",
    "    \"\"\"\n",
    "    # Vi·∫øt code c·ªßa b·∫°n ·ªü ƒë√¢y:\n",
    "    return 2*x -4\n",
    "\n",
    "# Test h√†m s·ªë (kh√¥ng c·∫ßn s·ª≠a)\n",
    "print(\"Test h√†m s·ªë:\")\n",
    "print(f\"f(0) = {f(0)} (k·∫øt qu·∫£ ƒë√∫ng: 7)\")\n",
    "print(f\"f(2) = {f(2)} (k·∫øt qu·∫£ ƒë√∫ng: 3)\")\n",
    "print(f\"df_dx(0) = {df_dx(0)} (k·∫øt qu·∫£ ƒë√∫ng: -4)\")\n",
    "print(f\"df_dx(2) = {df_dx(2)} (k·∫øt qu·∫£ ƒë√∫ng: 0)\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TODO 2.3:** Tri·ªÉn khai thu·∫≠t to√°n Gradient Descent\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO 2.3: Tri·ªÉn khai thu·∫≠t to√°n Gradient Descent\n",
    "def gradient_descent(func, grad_func, x_start, learning_rate, max_iterations, tolerance=1e-6):\n",
    "    \"\"\"\n",
    "    Thu·∫≠t to√°n Gradient Descent\n",
    "    \n",
    "    Args:\n",
    "        func: H√†m m·ª•c ti√™u c·∫ßn t·ªëi ∆∞u\n",
    "        grad_func: H√†m t√≠nh gradient (ƒë·∫°o h√†m)\n",
    "        x_start: ƒêi·ªÉm kh·ªüi t·∫°o\n",
    "        learning_rate: T·ªëc ƒë·ªô h·ªçc (alpha)\n",
    "        max_iterations: S·ªë v√≤ng l·∫∑p t·ªëi ƒëa\n",
    "        tolerance: Ng∆∞·ª°ng d·ª´ng (khi gradient ƒë·ªß nh·ªè)\n",
    "    \n",
    "    Returns:\n",
    "        dict: {\n",
    "            'x_history': danh s√°ch c√°c ƒëi·ªÉm x qua c√°c iteration,\n",
    "            'f_history': danh s√°ch gi√° tr·ªã h√†m s·ªë,\n",
    "            'gradient_history': danh s√°ch gradient,\n",
    "            'converged': True n·∫øu h·ªôi t·ª•, False n·∫øu kh√¥ng\n",
    "        }\n",
    "    \"\"\"\n",
    "    \n",
    "    # Kh·ªüi t·∫°o\n",
    "    x = x_start\n",
    "    x_history = [x]\n",
    "    f_history = [func(x)]\n",
    "    gradient_history = [grad_func(x)]\n",
    "    \n",
    "    # TODO: Vi·∫øt v√≤ng l·∫∑p gradient descent\n",
    "    for i in range(max_iterations):\n",
    "        # B∆∞·ªõc 1: T√≠nh gradient t·∫°i ƒëi·ªÉm hi·ªán t·∫°i\n",
    "        gradient = grad_func(x)\n",
    "        \n",
    "        # B∆∞·ªõc 2: Ki·ªÉm tra ƒëi·ªÅu ki·ªán d·ª´ng\n",
    "        if abs(gradient) < tolerance:\n",
    "            print(f\"H·ªôi t·ª• sau {i+1} iterations\")\n",
    "            return {\n",
    "                'x_history': x_history,\n",
    "                'f_history': f_history,\n",
    "                'gradient_history': gradient_history,\n",
    "                'converged': True\n",
    "            }\n",
    "        \n",
    "        # B∆∞·ªõc 3: C·∫≠p nh·∫≠t x theo c√¥ng th·ª©c gradient descent\n",
    "        x =x - learning_rate * gradient\n",
    "        \n",
    "        # B∆∞·ªõc 4: L∆∞u l·ªãch s·ª≠\n",
    "        x_history.append(x)\n",
    "        f_history.append(func(x))\n",
    "        gradient_history.append(gradient)\n",
    "    \n",
    "    print(f\"Kh√¥ng h·ªôi t·ª• sau {max_iterations} iterations\")\n",
    "    return {\n",
    "        'x_history': x_history,\n",
    "        'f_history': f_history,\n",
    "        'gradient_history': gradient_history,\n",
    "        'converged': False\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "H·ªôi t·ª• sau 84 iterations\n",
      "ƒêi·ªÉm h·ªôi t·ª•: 2.000000434220335\n",
      "Gi√° tr·ªã nh·ªè nh·∫•t: 3.0000000000001883\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "\n",
    "result = gradient_descent(f, df_dx, x_start=50, learning_rate=0.1, max_iterations=1000)\n",
    "\n",
    "print(\"ƒêi·ªÉm h·ªôi t·ª•:\", result['x_history'][-1])\n",
    "print(\"Gi√° tr·ªã nh·ªè nh·∫•t:\", result['f_history'][-1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
